{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e1b485-219c-493e-9b62-a427e4ab8686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d708bc1-1be5-46bb-813f-e9c7f31b2f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema of the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"EMPLOYEE_ID\", IntegerType(), True),\n",
    "    StructField(\"FIRST_NAME\", StringType(), True),\n",
    "    StructField(\"SALARY\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70951fe2-76c1-4844-85b6-1cd7fa08f80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory where files will be read from\n",
    "inputPath = \"dbfs:/mnt/streaming_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb24661-5a63-4eeb-affe-c4655f515b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read stream using Autoloader\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"dbfs:/mnt/streaming_data/schema\")\n",
    "    .schema(schema)\n",
    "    .load(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cc4783-c6a9-41a8-9eda-53dae7efe451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic transformation\n",
    "transformed_df = df.withColumn(\"SALARY_IN_K\", col(\"SALARY\") / 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7dff7c-41cb-4e53-b250-a0c97af758d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write stream to console (for debugging)\n",
    "query = (\n",
    "    transformed_df.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540838bf-ff61-43a6-9cc9-7d428d0417f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()  # Stop the existing stream\n",
    "# Restart the streaming query\n",
    "query = transformed_df.writeStream.format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8c8e70-daa5-4124-94c7-9b0b86495af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 122805 bytes.\nðŸ“‚ File /mnt/streaming_data/streaming_data_1742472925.csv with 5000 rows added!\nWrote 123915 bytes.\nðŸ“‚ File /mnt/streaming_data/streaming_data_1742472927.csv with 5000 rows added!\nWrote 123915 bytes.\nðŸ“‚ File /mnt/streaming_data/streaming_data_1742472929.csv with 5000 rows added!\nðŸš€ Streaming files have been created in /mnt/streaming_data_Auto!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to create a CSV file with streaming data\n",
    "def create_streaming_file(file_num, rows=5000):\n",
    "    csv_data = \"JOB_ID,FIRST_NAME,SALARY\\n\"  # CSV header\n",
    "    for i in range(rows):\n",
    "        csv_data += f\"{file_num * 1000 + i},Employee_{i},{50000 + (i % 10000)}\\n\"\n",
    "    \n",
    "    file_path = f\"/mnt/streaming_data/streaming_data_{int(time.time())}.csv\"\n",
    "    dbutils.fs.put(file_path, csv_data, True)\n",
    "    print(f\"ðŸ“‚ File {file_path} with {rows} rows added!\")\n",
    "\n",
    "# Generate multiple files to create a data spike\n",
    "for i in range(3):  # Creating 3 files\n",
    "    create_streaming_file(i, rows=5000)\n",
    "    time.sleep(2)  # Adding a small delay between file creations\n",
    "\n",
    "print(\"ðŸš€ Streaming files have been created in /mnt/streaming_data_Auto!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day3-Autoloader",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
