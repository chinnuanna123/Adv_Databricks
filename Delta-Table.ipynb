{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b5e6db-ab97-4cf7-9563-cbcf727e3ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  3|Cathy| 25|\n|  1|Alice| 30|\n|  2|  Bob| 28|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaTableDemo\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path for the Delta Table\n",
    "delta_table_path = \"/tmp/delta_table_demo\"\n",
    "\n",
    "# Create some sample data\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "data = [(1, \"Alice\", 30), (2, \"Bob\", 28), (3, \"Cathy\", 25)]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Read the data from the Delta Table\n",
    "print(\"Initial Data:\")\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c950a2-de20-4f3c-a98a-7228c8d823de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUpdate Transaction - Updating Bob's age to 29\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  3|Cathy| 25|\n|  1|Alice| 30|\n|  2|  Bob| 29|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Update Transaction\n",
    "print(\"\\nUpdate Transaction - Updating Bob's age to 29\")\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table.update(\"name = 'Bob'\", {\"age\": \"29\"})\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b22fbd2-b039-4cd9-81c8-9fd2ee31a982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDelete Transaction - Deleting Cathy\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  1|Alice| 30|\n|  2|  Bob| 29|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Delete Transaction\n",
    "print(\"\\nDelete Transaction - Deleting Cathy\")\n",
    "delta_table.delete(\"name = 'Cathy'\")\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2889ea5a-f2ca-4bff-8f5e-e79cb626e7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMerge Operation - Upserting new and modified data\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  4|David| 35|\n|  2|  Bob| 30|\n|  1|Alice| 30|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Merge Operation (Upsert Transaction)\n",
    "print(\"\\nMerge Operation - Upserting new and modified data\")\n",
    "new_data = [(2, \"Bob\", 30), (4, \"David\", 35)]\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "new_df.createOrReplaceTempView(\"updates\")\n",
    "delta_table.alias(\"t\").merge(\n",
    "    new_df.alias(\"u\"),\n",
    "    \"t.id = u.id\"\n",
    ")\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fa1fc8-0dc0-43cd-ba7c-6d01c77e27a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTime Travel - Reading previous version\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  3|Cathy| 25|\n|  1|Alice| 30|\n|  2|  Bob| 28|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Time Travel (Reading Old Data)\n",
    "print(\"\\nTime Travel - Reading previous version\")\n",
    "old_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "old_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc71556-8702-443f-91aa-d1aa850e4967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTime Travel - Reading previous version\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  1|Alice| 30|\n|  2|  Bob| 29|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Time Travel (Reading Old Data)\n",
    "print(\"\\nTime Travel - Reading previous version\")\n",
    "old_df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(delta_table_path)\n",
    "old_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b8f871-c73a-49dc-8462-5ea85fe13307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_table_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b50d6b-ea11-494c-b33e-b3e680399ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE delta_table_demo\n",
    "    ZORDER BY (age)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b576a6-6d8f-4e15-9b4a-fa19597ac1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------------+----------+\n|format|                  id|                name|description|            location|           createdAt|       lastModified|partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|       tableFeatures|statistics|\n+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------------+----------+\n| delta|35fd1672-8399-4c0...|spark_catalog.def...|       NULL|dbfs:/user/hive/w...|2025-03-18 12:59:...|2025-03-18 13:04:31|              []|               []|       1|       1201|        {}|               1|               2|[appendOnly, inva...|        {}|\n+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE DETAIL delta_table_demo\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta-Table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
